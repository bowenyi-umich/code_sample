{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c8d8ed-83c0-41a6-83ab-ff2c91c83998",
   "metadata": {},
   "source": [
    "#### In this notebook, I will prepare the train, validation, and test sets for logistic regression and naive bayes models. I will train them on (unigrams, bigrams) and (bigrams, trigrams) and measure their performance on accuracy and F-1 score. \n",
    "\n",
    "#### Our class will be made balanced deliberately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af4fefd9-9ec1-439c-b779-b2ca3f1fad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"/home/bowenyi/.local/lib/python3.11/site-packages\")\n",
    "!pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bfc135ad-68ef-44ac-875f-3db7642afc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/processed/beforeFloydMonth/beforeFMonth.tsv\", lineterminator = '\\n', low_memory=False)\n",
    "df_in = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/processed/floydMonth/floydMonthEnSHORT.csv\", lineterminator = '\\n', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8af822-8678-4ebf-b93a-7f4dd541d2a5",
   "metadata": {},
   "source": [
    "##### *Preprocessing the dataframes by dropping duplicate rows*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "009126e8-331d-4c8b-8bef-042525761355",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = df_before.dropna(subset=['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10'], how='all')\n",
    "df_before = df_before.drop_duplicates()\n",
    "df_before = df_before.drop_duplicates(subset=['potentialOutPath'])\n",
    "\n",
    "df_in = df_in.dropna(subset=['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10'], how='all')\n",
    "df_in = df_in.drop_duplicates()\n",
    "df_in = df_in.drop_duplicates(subset=['potentialOutPath'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9ac927-3107-4591-9dca-a4a83c38132c",
   "metadata": {},
   "source": [
    "##### Introduce two columns that represent data labels: is_news and is_politics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7f3a67ed-2d0a-4075-80a6-ce193614483b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdd two columns to df_after\\n'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Add two columns to the existing dataframes: \"is_news\", \"is_politics\". \n",
    "\n",
    "is_news and is_politics take the value of 1 or 0. \n",
    "\n",
    "is_politics is 1 if \"politics\" or \"government\" is one of the categories. According to Ashwin's 2021 paper \n",
    "Political Discussion is Abundant..., a podcast is political if it is about political institution (definition modified). \n",
    "Thus, if a podcast is categorized as \"government\", it is political.\n",
    "'''\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "df_before['is_news'] = df_before[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if 'news' in x.values else 0, axis=1)\n",
    "df_before['is_politics'] = df_before[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if ('politics' in x.values or 'government' in x.values) else 0, axis=1)\n",
    "\n",
    "df_in['is_news'] = df_in[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if 'news' in x.values else 0, axis=1)\n",
    "df_in['is_politics'] = df_in[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if ('politics' in x.values or 'government' in x.values) else 0, axis=1)\n",
    "\n",
    "#TODO: Add two columns to df_after\n",
    "df_after['is_news'] = df_after[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if 'news' in x.values else 0, axis=1)\n",
    "df_after['is_politics'] = df_after[['category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10']].apply(lambda x: 1 if ('politics' in x.values or 'government' in x.values) else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d9b87066-f709-4e4d-be42-fcf57c69ecc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMethod 2 (undecided): Cross-validation\\nMaintain 15% data as heldout sets for is_news and is_politics. The rest 85% are for cross-validation. \\nEach classifier has two test/heldout sets: test_cv_pol, test_inFMonth_pol | test_cv_news, test_inFMonth_news\\n'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2. Set the ratio for politics/news vs. non-politics/news to 1:1. \n",
    "'''\n",
    "# Before Floyd Month:\n",
    "#is_politics:\n",
    "df_before_pol = df_before[df_before['is_politics'] == 1].copy()\n",
    "n_before_pol = df_before_pol.shape[0]\n",
    "df_before_no_pol = df_before[df_before['is_politics'] == 0].sample(n=n_before_pol, replace=False, random_state=387).copy()\n",
    "#is_news:\n",
    "df_before_news = df_before[df_before['is_news'] == 1].copy()\n",
    "n_before_news = df_before_news.shape[0]\n",
    "df_before_no_news = df_before[df_before['is_news'] == 0].sample(n=n_before_news, replace=False, random_state=387).copy()\n",
    "\n",
    "# In Floyd Month:\n",
    "#is_politics:\n",
    "df_in_pol = df_in[df_in['is_politics'] == 1].copy()\n",
    "n_in_pol = df_in_pol.shape[0]\n",
    "df_in_no_pol = df_in[df_in['is_politics'] == 0].sample(n=n_in_pol, replace=False, random_state=387).copy()\n",
    "#is_news:\n",
    "df_in_news = df_in[df_in['is_news'] == 1].copy()\n",
    "n_in_news = df_in_news.shape[0]\n",
    "df_in_no_news = df_in[df_in['is_news'] == 0].sample(n=n_in_news, replace=False, random_state=387).copy()\n",
    "\n",
    "# TODO: After Floyd Month \n",
    "# df_after_pol = df_after[df_after['is_politics'] == 1].copy()\n",
    "# n_after_pol = df_after_pol.shape[0]\n",
    "# df_after_no_pol = df_after[df_after['is_politics'] == 0].sample(n=n_after_pol, replace=False, random_state=387).copy()\n",
    "# #is_news:\n",
    "# df_after_news = df_after[df_after['is_news'] == 1].copy()\n",
    "# n_after_news = df_after_news.shape[0]\n",
    "# df_after_no_news = df_after[df_after['is_news'] == 0].sample(n=n_after_news, replace=False, random_state=387).copy()\n",
    "\n",
    "'''\n",
    "Two test sets, for each of politics and news \n",
    "'''\n",
    "# Politics:\n",
    "df_train_before_pol = pd.concat([df_before_pol, df_before_no_pol], ignore_index=True)\n",
    "df_train_before_pol = df_train_before_pol.sample(frac=1, random_state=387).reset_index(drop=True)\n",
    "\n",
    "df_train_after_pol = pd.concat([df_after_pol, df_after_no_pol], ignore_index=True)\n",
    "df_train_after_pol = df_train_after_pol.sample(frac=1, random_state=387).reset_index(drop=True)\n",
    "\n",
    "# The second test set inFMonth\n",
    "df_test_in_pol = pd.concat([df_in_pol, df_in_no_pol], ignore_index=True).sample(frac=1, random_state=387).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# News:\n",
    "df_train_before_news = pd.concat([df_before_news, df_before_no_news], ignore_index=True)\n",
    "df_train_before_news = df_train_before_news.sample(frac=1, random_state=387).reset_index(drop=True)\n",
    "\n",
    "df_train_after_news = pd.concat([df_after_news, df_after_no_news], ignore_index=True)\n",
    "df_train_after_news = df_train_after_news.sample(frac=1, random_state=387).reset_index(drop=True)\n",
    "# The second test set inFMonth\n",
    "df_test_in_news = pd.concat([df_in_news, df_in_no_news], ignore_index=True).sample(frac=1, random_state=387).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# train_pol, dev_test_pol = train_test_split(df_train_pol, test_size=0.3, random_state=387)\n",
    "# dev_pol, test_before_after_pol = train_test_split(dev_test_pol, test_size=2/3, random_state=387)\n",
    "# test_in_pol = pd.concat([df_in_pol, df_in_no_pol], ignore_index=True).sample(frac=1, random_state=387).reset_index(drop=True)\n",
    "\n",
    "# train_news, dev_test_news = train_test_split(df_train_news, test_size=0.3, random_state=387)\n",
    "# dev_news, test_before_after_news = train_test_split(dev_test_news, test_size=2/3, random_state=387)\n",
    "# test_in_news = pd.concat([df_in_news, df_in_no_news], ignore_index=True).sample(frac=1, random_state=387).reset_index(drop=True)\n",
    "\n",
    "'''\n",
    "Method 2 (undecided): Cross-validation\n",
    "Maintain 15% data as heldout sets for is_news and is_politics. The rest 85% are for cross-validation. \n",
    "Each classifier has two test/heldout sets: test_cv_pol, test_inFMonth_pol | test_cv_news, test_inFMonth_news\n",
    "'''\n",
    "# # politics:\n",
    "# df_train_pol = pd.concat([df_before_pol, df_before_no_pol, df_after_pol, df_after_no_pol], ignore_index=True)\n",
    "# df_train_pol = df_train_pol.sample(frac=1, random_state=387).reset_index(drop=True)\n",
    "# train_pol, test_cv_pol = train_test_split(df_train_pol, test_size=0.15, random_state=387)\n",
    "# test_inFMonth_pol = pd.concat([df_in_pol, df_in_no_pol], ignore_index=True).sample(frac=1, random_state=387).reset_index(drop=True)\n",
    "\n",
    "# # news:\n",
    "# df_train_news = pd.concat([df_before_news, df_before_no_news, df_after_news, df_after_no_news], ignore_index=True)\n",
    "# df_train_news = df_train_news.sample(frac=1, random_state=387).reset_index(drop=True)\n",
    "# train_news, test_cv_news = train_test_split(df_train_news, test_size=0.15, random_state=387)\n",
    "# test_inFMonth_news = pd.concat([df_in_news, df_in_no_news], ignore_index=True).sample(frac=1, random_state=387).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "7485631d-653a-418d-9a56-62337cc7f59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/bowenyi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Cross-Validation pipeline for is_news and is_politics: \n",
    "1. Split the dataset (non-inFMonth) into 85% for cross-validation (cv) and 15% for test. \n",
    "2. Intiate three models (Logistic Regression, Multinomial Naive Bayes, SVM) and a range parameters (RandomSearch)\n",
    "3. Pick the model and their parameters with the best performance on cv  \n",
    "3. Train the model on the entire 85% dataset that we use for cv\n",
    "4. Evaluate the model performance on two test sets: test_cv (15% of the dataset) and test_inFMonth (inFMonth data)\n",
    "'''\n",
    "# Helper 1: get the file path for beforeFloydMonth data\n",
    "def get_path_before(potentialOutPath):\n",
    "    rootPath = \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/beforeFMonth\"\n",
    "    return rootPath + potentialOutPath\n",
    "\n",
    "# Helper 2: get the file path for FloydMonth data\n",
    "def get_path_in(potentialOutPath):\n",
    "    rootPath = \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/floydMonth\"\n",
    "    return rootPath + potentialOutPath\n",
    "\n",
    "'''\n",
    "TODO Helper 3: get the file path for afterFloydMonth data\n",
    "'''\n",
    "def get_path_after(potentialOutPath):\n",
    "    # rootPath = \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/afterFMonth\"\n",
    "    # return rootPath + potentialOutPath\n",
    "    pass\n",
    "\n",
    "#Helper 4: Preprocess text \n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import contractions\n",
    "from urllib.parse import urlparse\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords.add('u')\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return wordnet.NOUN\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"\\n\",\"\",text)   #remove line breaks\n",
    "    text = re.sub(r'\\[.*?\\]', '', text) # remove [Music], (Audio), etc.\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    text = text.lower()    #convert to lowercase\n",
    "    text = re.sub(r'\\b\\w+\\.com\\b', '', text) #remove something.com\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)   #remove hyperlinks\n",
    "    text = re.sub(r\"\\d+\",\"\",text)   #remove digits and currencies \n",
    "    text = re.sub(r'[\\$\\d+\\d+\\$]', \"\", text)\n",
    "    text = re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', text)   #remove dates \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)   #remove non-ascii\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)   #remove punctuation\n",
    "    \n",
    "    filtered_tokens = [word for word in word_tokenize(text) if not word in stopwords]\n",
    "    # Lemmatization\n",
    "    pos_tags = pos_tag(filtered_tokens)  \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags if lemmatizer.lemmatize(word, get_wordnet_pos(tag)) not in stopwords]\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    return lemmatized_text\n",
    "    \n",
    "def split_transcript(transcript):\n",
    "    transcript['content'] = transcript['content'].fillna('').astype(str)\n",
    "    start_time = 0\n",
    "    chunks = [] # List of lists. Each sub-list is a string of chunked text  \n",
    "    chunk = \"\"\n",
    "    end_of_sentence = ['.', '!', '?', ']', ')']\n",
    "    \n",
    "    for index, row in transcript.iterrows():\n",
    "        content = str(row['content'])\n",
    "        if content.strip() != '':\n",
    "            if row['end'] - start_time < 60:\n",
    "                chunk += content\n",
    "            else:\n",
    "                chunk += content\n",
    "                if any(ele in content for ele in end_of_sentence): # if we reach the end of a sentence\n",
    "                    chunk = preprocess_text(chunk)\n",
    "                    if chunk.strip() != '': # ignore chunks that are only white spaces or empty\n",
    "                        list = []\n",
    "                        list.append(chunk)\n",
    "                        chunks.append(list)\n",
    "                        start_time = row['end']\n",
    "                    chunk = \"\"           \n",
    "\n",
    "    if len(chunk) != 0:\n",
    "        chunk = preprocess_text(chunk)\n",
    "        if chunk.strip() != '':\n",
    "            list = []\n",
    "            list.append(chunk)\n",
    "            chunks.append(list)\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1eeaf-c20f-422d-997e-8d68f3965ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4. Prepare train, validate, test sets: is_pol and is_news\n",
    "'''\n",
    "import os\n",
    "\n",
    "x_before_after_pol = []  \n",
    "y_before_after_pol = [] \n",
    "x_before_after_news = []  \n",
    "y_before_after_news = [] \n",
    "\n",
    "x_in_pol = []\n",
    "y_in_pol = []\n",
    "x_in_news = []\n",
    "y_in_news = []\n",
    "\n",
    "# Read training data\n",
    "# is_pol\n",
    "for index, row in df_train_before_pol.iterrows():\n",
    "    transcript_path = get_path_before(row[\"potentialOutPath\"])\n",
    "    if os.path.isfile(transcript_path):\n",
    "        transcript = pd.read_csv(transcript_path, usecols=['start', 'end', 'content'])\n",
    "        chunks = split_transcript(transcript)\n",
    "        x_before_after_pol.extend(chunks)\n",
    "        y_before_after_pol.extend([row[\"is_politics\"]] * len(chunks))\n",
    "\n",
    "for index, row in df_train_after_pol.iterrows():\n",
    "    transcript_path = get_path_after(row[\"potentialOutPath\"])\n",
    "    if os.path.isfile(transcript_path):\n",
    "        transcript = pd.read_csv(transcript_path, usecols=['start', 'end', 'content'])\n",
    "        chunks = split_transcript(transcript)\n",
    "        x_before_after_pol.extend(chunks)\n",
    "        y_before_after_pol.extend([row[\"is_politics\"]] * len(chunks))\n",
    "        \n",
    "# is_news: \n",
    "for index, row in df_train_before_news.iterrows():\n",
    "    transcript_path = get_path_before(row[\"potentialOutPath\"])\n",
    "    if os.path.isfile(transcript_path):\n",
    "        transcript = pd.read_csv(transcript_path, usecols=['start', 'end', 'content'])\n",
    "        chunks = split_transcript(transcript)\n",
    "        x_before_after_news.extend(chunks)\n",
    "        y_before_after_news.extend([row[\"is_news\"]] * len(chunks))\n",
    "\n",
    "for index, row in df_train_after_news.iterrows():\n",
    "    transcript_path = get_path_after(row[\"potentialOutPath\"])\n",
    "    if os.path.isfile(transcript_path):\n",
    "        transcript = pd.read_csv(transcript_path, usecols=['start', 'end', 'content'])\n",
    "        chunks = split_transcript(transcript)\n",
    "        x_before_after_news.extend(chunks)\n",
    "        y_before_after_news.extend([row[\"is_news\"]] * len(chunks))\n",
    "             \n",
    "# Read second test set (in FMonth)\n",
    "for index, row in df_test_in_pol.iterrows():  \n",
    "    transcript_path = get_path_in(row[\"potentialOutPath\"])\n",
    "    if os.path.isfile(transcript_path):\n",
    "        transcript = pd.read_csv(transcript_path, usecols=['start', 'end', 'content'])\n",
    "        chunks = split_transcript(transcript)\n",
    "        x_in_pol.extend(chunks)\n",
    "        y_in_pol.extend([row[\"is_politics\"]] * len(chunks))\n",
    "\n",
    "for index, row in df_test_in_news.iterrows():  \n",
    "    transcript_path = get_path_in(row[\"potentialOutPath\"])\n",
    "    if os.path.isfile(transcript_path):\n",
    "        transcript = pd.read_csv(transcript_path, usecols=['start', 'end', 'content'])\n",
    "        chunks = split_transcript(transcript)\n",
    "        x_in_news.extend(chunks)\n",
    "        y_in_news.extend([row[\"is_news\"]] * len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8d7ac-28e6-4db7-8a72-3c15d983c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5. Split data into train, validation, and test sets for is_pol and is_news.\n",
    "\n",
    "Note: we have two test sets. x_in_news/pol and x_test_news/pol. \n",
    "'''\n",
    "#is_pol:\n",
    "x_train_pol, x_other_pol, y_train_pol, y_other_pol = train_test_split(\n",
    "    x_before_after_pol, y_before_after_pol, test_size=0.3, random_state=387)\n",
    "\n",
    "x_dev_pol, x_test_pol, y_dev_pol, y_test_pol = train_test_split(\n",
    "    x_other_pol, y_other_pol, test_size=(2/3), random_state=387)\n",
    "\n",
    "#is_news:\n",
    "x_train_news, x_other_news, y_train_news, y_other_news = train_test_split(\n",
    "    x_before_after_news, y_before_after_news, test_size=0.3, random_state=387)\n",
    "\n",
    "x_dev_news, x_test_news, y_dev_news, y_test_news = train_test_split(\n",
    "    x_other_news, y_other_news, test_size=(2/3), random_state=387)\n",
    "\n",
    "'''\n",
    "is_pol: \n",
    "- train: x_train_pol, y_train_pol\n",
    "- dev: x_dev_pol, y_dev_pol\n",
    "- test: x_test_pol, y_test_pol; x_in_pol, y_in_pol \n",
    "\n",
    "\n",
    "is_news: \n",
    "- train: x_train_news, y_train_news\n",
    "- dev: x_dev_news, y_dev_news\n",
    "- test: x_test_news, y_test_news; x_in_news, y_in_news\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ffa90-dffd-44d1-9a04-db4925f47170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data for training\n",
    "# Ngrams = (1, 2) and (2, 3)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect_12 = CountVectorizer(ngram_range=(1, 2))\n",
    "count_vect_23 = CountVectorizer(ngram_range=(2, 3))\n",
    "\n",
    "#is_pol:\n",
    "#ngrams=(1,2):\n",
    "x_train_12_pol = count_vect_12.fit_transform(x_train_pol)\n",
    "x_dev_12_pol  = count_vect_12.transform(x_dev_pol)\n",
    "x_test_12_pol  = count_vect_12.transform(x_test_pol)\n",
    "x_in_12_pol = count_vect_12.transform(x_in_pol)\n",
    "\n",
    "#ngrams=(2,3):\n",
    "x_train_23_pol = count_vect_23.fit_transform(x_train_pol)\n",
    "x_dev_23_pol  = count_vect_23.transform(x_dev_pol)\n",
    "x_test_23_pol  = count_vect_23.transform(x_test_pol)\n",
    "x_in_23_pol = count_vect_23.transform(x_in_pol)\n",
    "\n",
    "\n",
    "#is_news:\n",
    "#ngrams=(1,2):\n",
    "x_train_12_news = count_vect_12.fit_transform(x_train_news)\n",
    "x_dev_12_news  = count_vect_12.transform(x_dev_news)\n",
    "x_test_12_news  = count_vect_12.transform(x_test_news)\n",
    "x_in_12_news = count_vect_12.transform(x_in_news)\n",
    "\n",
    "#ngrams=(2,3):\n",
    "x_train_23_news = count_vect_23.fit_transform(x_train_news)\n",
    "x_dev_23_news  = count_vect_23.transform(x_dev_news)\n",
    "x_test_23_news  = count_vect_23.transform(x_test_news)\n",
    "x_in_23_news = count_vect_23.transform(x_in_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47bc2e-c553-442b-8cbb-93456a496988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store variables so that they can be accessed in other documents\n",
    "'''\n",
    "is_pol: \n",
    "- train: x_train_pol, y_train_pol\n",
    "- dev: x_dev_pol, y_dev_pol\n",
    "- test: x_test_pol, y_test_pol; x_in_pol, y_in_pol \n",
    "\n",
    "\n",
    "is_news: \n",
    "- train: x_train_news, y_train_news\n",
    "- dev: x_dev_news, y_dev_news\n",
    "- test: x_test_news, y_test_news; x_in_news, y_in_news\n",
    "'''\n",
    "%store x_train_pol\n",
    "%store y_train_pol\n",
    "%store x_dev_pol\n",
    "%store y_dev_pol\n",
    "%store x_test_pol\n",
    "%store y_test_pol\n",
    "%store x_in_pol\n",
    "%store y_in_pol\n",
    "    \n",
    "%store x_train_news\n",
    "%store y_train_news\n",
    "%store x_dev_news\n",
    "%store y_dev_news\n",
    "%store x_test_news\n",
    "%store y_test_news\n",
    "%store x_in_news\n",
    "%store y_in_news\n",
    "\n",
    "#is_pol\n",
    "#ngrams=(1,2)\n",
    "%store x_train_12_pol \n",
    "%store x_dev_12_pol  \n",
    "%store x_test_12_pol \n",
    "%store x_in_12_pol\n",
    "\n",
    "#ngrams=(2,3):\n",
    "%store x_train_23_pol \n",
    "%store x_dev_23_pol  \n",
    "%store x_test_23_pol  \n",
    "%store x_in_23_pol \n",
    "\n",
    "#is_news:\n",
    "#ngrams=(1,2):\n",
    "%store x_train_12_news \n",
    "%store x_dev_12_news  \n",
    "%store x_test_12_news  \n",
    "%store x_in_12_news \n",
    "\n",
    "#ngrams=(2,3):\n",
    "%store x_train_23_news \n",
    "%store x_dev_23_news \n",
    "%store x_test_23_news  \n",
    "%store x_in_23_news "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26eb70a-4fb1-4279-9810-4b2e6a51f417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
