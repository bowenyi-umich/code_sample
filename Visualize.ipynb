{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d4b4d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58341a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c4d017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b08b24ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60c3bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "282974f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d265eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "d15c1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "242bc7e2-c066-401a-960e-054cb260fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/processed/beforeFloydMonth/beforeFMonth.tsv\", lineterminator='\\n', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8eec4617-3b29-4730-9ff8-b3a9358dca01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'rssUrl', 'epTitle', 'epDescription', 'duration',\n",
       "       'pubDate', 'copyright', 'itunes:type', 'itunes:complete', 'guid',\n",
       "       'itunes:explicit', 'enclosure', 'itunes:image', 'transDict', 'id',\n",
       "       'title', 'lastUpdate', 'link', 'lastHttpStatus', 'dead', 'contentType',\n",
       "       'itunesId', 'originalUrl', 'itunesAuthor', 'itunesOwnerName',\n",
       "       'explicit', 'imageUrl', 'itunesType', 'generator', 'newestItemPubdate',\n",
       "       'language', 'oldestItemPubdate', 'episodeCount', 'popularityScore',\n",
       "       'priority', 'createdOn', 'updateFrequency', 'chash', 'host',\n",
       "       'newestEnclosureUrl', 'podcastGuid', 'podDescription', 'category1',\n",
       "       'category2', 'category3', 'category4', 'category5', 'category6',\n",
       "       'category7', 'category8', 'category9', 'category10',\n",
       "       'newestEnclosureDuration', 'oldestItemDatetime', 'cleanDates',\n",
       "       'potentialOutPath', 'cleanDatesLoc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ed8e32f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/shared/3/projects/benlitterer/podcastData/processed/allEpisodes.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m         nrows\n\u001b[1;32m   1706\u001b[0m     )\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread(nrows)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:796\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:884\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:2029\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/processed/allEpisodes.csv\", lineterminator='\\n', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "94cd934e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'queryName', 'title', 'url', 'originalUrl', 'description',\n",
       "       'author', 'language', 'categories', 'explicit_x', 'episodeCount',\n",
       "       'epTitle', 'epDescription', 'duration', 'pubDate', 'copyright', 'type',\n",
       "       'complete', 'guid', 'explicit_y', 'mp3Url', 'image', 'transDict',\n",
       "       'date_formats_with_offset', 'date_formats_without_offset',\n",
       "       'date_formats_without_PDT', 'date_format_same',\n",
       "       'date_formate_same_final', 'date_format_same_final'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "acb39996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[df['title'] == '上野誠の万葉歌ごよみ']['language'])\n",
    "# print(df[df['title'] == '2017-07-31 21:59:35.00']['ex'])\n",
    "# df['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "6270163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns\n",
    "# df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0021e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the explicit_y column. It contains a lot of inconsistent and confusing stuff.\n",
    "# Will use explicit_x column for analysis below. Still helpful to clean explicit_y \n",
    "mapping = {\n",
    "    'No': False,\n",
    "    'no': False,\n",
    "    'false': False,\n",
    "    'clean': False,\n",
    "    'yes': True,\n",
    "    'true': True,\n",
    "    'Yes': True,\n",
    "    'NO': False,\n",
    "    'False': False,\n",
    "    'Clean': False,\n",
    "    '    No': False,\n",
    "    'N': False,\n",
    "    'explicit': True,\n",
    "    'YES': True,\n",
    "    'Explicit': True,\n",
    "    'off': False,\n",
    "    'FALSE': False,\n",
    "    'falses': False\n",
    "}\n",
    "\n",
    "# Apply the mapping to the explicit_y column\n",
    "df['explicit_y'] = df['explicit_y'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd385604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Clean date format: ----------\n",
    "\n",
    "# Date formats:\n",
    "date_formats_with_offset = \"%a, %d %b %Y %H:%M:%S %z\"\n",
    "date_formats_without_offset = \"%a, %d %b %Y %H:%M:%S %Z\"\n",
    "date_format_without_tzone = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "# clean date using only one format, leaving NaTs for unmatched format \n",
    "df['date_formats_with_offset'] = pd.to_datetime(df['pubDate'], errors='coerce', format=date_formats_with_offset, utc=True).dt.tz_localize(None)\n",
    "df[\"date_formats_without_offset\"] = pd.to_datetime(df['pubDate'], errors='coerce', format=date_formats_without_offset, utc=True).dt.tz_localize(None)\n",
    "df[\"date_formats_without_PDT\"] = pd.to_datetime(df['pubDate'].replace({'PDT':'-07:00','PST':'-08:00'}, regex=True), errors='coerce', utc=True).dt.tz_localize(None)\n",
    "\n",
    "\n",
    "# Merge 3 columns  \n",
    "df[\"date_format_same\"] = df['date_formats_with_offset'].fillna(df[\"date_formats_without_offset\"])\n",
    "df['date_format_same_final'] = df[\"date_formats_without_PDT\"].fillna(df[\"date_format_same\"])\n",
    "\n",
    "# -------- date format cleaned ----------\n",
    "\n",
    "\n",
    "\n",
    "# # checks the amount of NaT\n",
    "# count_of_NaT = df['date_format_same_final'].isna().sum()\n",
    "# print(\"Amount of NaT values:\",count_of_NaT)\n",
    "# # the indices of NaT\n",
    "# indices_of_NaT = df.index[df['date_format_same_final'].isna()].tolist()\n",
    "# # print(\"Indices of NaT values:\", indices_of_NaT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aacabd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks the amount of NaT\n",
    "print(df.shape)\n",
    "count_of_NaT = df['date_format_same_final'].isna().sum()\n",
    "print(\"Amount of NaT values:\",count_of_NaT)\n",
    "# the indices of NaT\n",
    "indices_of_NaT = df.index[df['date_format_same_final'].isna()].tolist()\n",
    "# print(\"Indices of NaT values:\", indices_of_NaT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69181996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out inconsistent podcasts\n",
    "Df = pd.DataFrame()\n",
    "Df['podcast title'] = df['title']\n",
    "Df['publish date'] = df['date_format_same_final']\n",
    "Df['category'] = df['categories']\n",
    "Df['language'] = df['language']\n",
    "Df['explicit'] = df['explicit_x']\n",
    "\n",
    "Df = Df.dropna(subset=['publish date'])\n",
    "\n",
    "grouped = Df.groupby(['podcast title', 'category', 'language', 'explicit'])['publish date'].agg(['min', 'max', 'median'])\n",
    "grouped['time span'] = grouped['max'] - grouped['min']\n",
    "grouped['median - min'] = abs(grouped['median'] - grouped['min'])\n",
    "grouped['max - median'] = abs(grouped['max'] - grouped['median'])\n",
    "\n",
    "# Set it to 5500D because the first \n",
    "consistent_podcasts = grouped[(grouped['median - min'] < pd.Timedelta('7000D')) & \n",
    "                             (grouped['max - median'] < pd.Timedelta('7000D'))]\n",
    "\n",
    "consistent_sorted_podcasts = consistent_podcasts.sort_values('time span', ascending=False)\n",
    "consistent_sorted_podcasts = consistent_sorted_podcasts.reset_index()\n",
    "\n",
    "# Generate a CSV file that only has consistent podcasts:\n",
    "# consistent_sorted_podcasts.to_csv('consistent_podcasts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e06e82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECDF plot (y-axis = proportion)\n",
    "plt.figure(figsize=(10, 6))\n",
    "start_dates = consistent_sorted_podcasts['min'].dt.year\n",
    "# start_dates[0:10]\n",
    "sns.ecdfplot(x=start_dates)\n",
    "plt.savefig(\"year_data_ecdf_proportion.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c99d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECDF plot (y-axis = count)\n",
    "plt.figure(figsize=(10, 6))\n",
    "start_dates = consistent_sorted_podcasts['min'].dt.year\n",
    "# start_dates[0:10]\n",
    "sns.ecdfplot(x=start_dates, stat='count')\n",
    "plt.savefig(\"year_data_ecdf_proportion_count.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f44dbd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------- Plot ------------------------ \n",
    "# Plot 1,2,3: the most popular languages, categories, and explicit statuses during each presidency\n",
    "# Plot 4: most popular categories of all time\n",
    "\n",
    "\n",
    "# Get the unique titles in consistent_sorted_podcasts\n",
    "consistent_titles = consistent_sorted_podcasts['podcast title']\n",
    "\n",
    "# Filter df to only include rows with these titles\n",
    "df_clean = Df[Df['podcast title'].isin(consistent_titles)]\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "# Plot 1: the 30 most popular podcast categories during each US presidency\n",
    "\n",
    "# gov_periods = {\n",
    "#     'Bill Clinton': ('1993-01-20', '2001-01-20'),\n",
    "#     'George W. Bush': ('2001-01-20', '2009-01-20'),\n",
    "#     'Barack Obama': ('2009-01-20', '2017-01-20'),\n",
    "#     'Donald Trump': ('2017-01-20', '2021-01-20'),\n",
    "#     'Joe Biden': ('2021-01-20', 'today')\n",
    "# }\n",
    "# # Analyze each government period\n",
    "# for name, (start, end) in gov_periods.items():\n",
    "#     # Filter the data for the current government period\n",
    "#     mask = (df_clean['publish date'] >= start) & (df_clean['publish date'] < end)\n",
    "#     subset = df_clean[mask]\n",
    "\n",
    "#     # Count the number of podcasts in each category\n",
    "#     category_counts = subset['category'].value_counts()\n",
    "\n",
    "#     # Get the 30 most popular categories\n",
    "#     top_categories = category_counts[:30]\n",
    "\n",
    "#     # Check if top_categories is empty\n",
    "#     if not top_categories.empty:\n",
    "#         # Create a new figure\n",
    "#         plt.figure(figsize=(40, 10))\n",
    "\n",
    "#         # Create a bar plot using seaborn\n",
    "#         sns.barplot(x=top_categories.values, y=top_categories.index)\n",
    "\n",
    "#         # Add labels and title\n",
    "#         plt.title(f'Top 30 Podcast Categories during {name}\\'s Presidency')\n",
    "#         plt.xlabel('Number of Podcasts')\n",
    "#         plt.ylabel('Category')\n",
    "\n",
    "#         filename = f\"{name.replace(' ', '_')}_popular_categories.png\"\n",
    "#         plt.savefig(filename)\n",
    "\n",
    "#         plt.close()\n",
    "#     else:\n",
    "#         print(f\"No podcasts found during {name}'s presidency.\")  \n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "# Plot 2: the 30 most popular podcast languages during each US presidency\n",
    "# gov_periods = {\n",
    "#     'Bill Clinton': ('1993-01-20', '2001-01-20'),\n",
    "#     'George W. Bush': ('2001-01-20', '2009-01-20'),\n",
    "#     'Barack Obama': ('2009-01-20', '2017-01-20'),\n",
    "#     'Donald Trump': ('2017-01-20', '2021-01-20'),\n",
    "#     'Joe Biden': ('2021-01-20', 'today')\n",
    "# }\n",
    "# # Analyze each government period\n",
    "# for name, (start, end) in gov_periods.items():\n",
    "#     # Filter the data for the current government period\n",
    "#     mask = (df_clean['publish date'] >= start) & (df_clean['publish date'] < end)\n",
    "#     subset = df_clean[mask]\n",
    "\n",
    "#     # Count the number of podcasts in each category\n",
    "#     language_counts = subset['language'].value_counts()\n",
    "\n",
    "#     # Get the 30 most popular categories\n",
    "#     top_languages = language_counts[:30]\n",
    "\n",
    "    # Check if top_categories is empty\n",
    "#     if not top_languages.empty:\n",
    "#         # Create a new figure\n",
    "#         plt.figure(figsize=(40, 10))\n",
    "\n",
    "#         # Create a bar plot using seaborn\n",
    "#         sns.barplot(x=top_languages.values, y=top_languages.index)\n",
    "\n",
    "#         # Add labels and title\n",
    "#         plt.title(f'Top 30 Podcast Languages during {name}\\'s Presidency')\n",
    "#         plt.xlabel('Number of Podcasts')\n",
    "#         plt.ylabel('Language')\n",
    "\n",
    "#         filename = f\"{name.replace(' ', '_')}_popular_languages.png\"\n",
    "#         plt.savefig(filename)\n",
    "\n",
    "#         plt.close()\n",
    "#     else:\n",
    "#         print(f\"No podcasts found during {name}'s presidency.\")  \n",
    "  \n",
    "\n",
    "## --------------------------------------------------------------------------\n",
    "\n",
    "# Plot 3: the populaity of explicit podcast during each presidency\n",
    "\n",
    "# Won't set X-axis to the same metric because we won't be able to observe anything in Bush and Clinton's presidency by doing this\n",
    "\n",
    "# However, it's a good starting point. I've commented out the code to set X-axis's limit to the highest frequency, come back if needed\n",
    "\n",
    "# gov_periods = {\n",
    "#     'Bill Clinton': ('1993-01-20', '2001-01-20'),\n",
    "#     'George W. Bush': ('2001-01-20', '2009-01-20'),\n",
    "#     'Barack Obama': ('2009-01-20', '2017-01-20'),\n",
    "#     'Donald Trump': ('2017-01-20', '2021-01-20'),\n",
    "#     'Joe Biden': ('2021-01-20', 'today')\n",
    "# }\n",
    "\n",
    "# --------- Required part to set X-axis upper limit to the highest freq\n",
    "# highest_freq = 0\n",
    "# # Get the highest frequency and use it as the upper limit of the X-axis\n",
    "# for name, (start, end) in gov_periods.items():\n",
    "#     # Filter the data for the current government period\n",
    "#     mask = (df_clean['publish date'] >= start) & (df_clean['publish date'] < end)\n",
    "#     subset = df_clean[mask]\n",
    "\n",
    "#     # Count the number of explicit and non-explicit podcasts\n",
    "#     explicit_counts = subset['explicit'].value_counts()\n",
    "#     highest_freq = max(highest_freq, explicit_counts.iloc[0])\n",
    "# --------- Required part to set X-axis upper limit to the highest freq    \n",
    "\n",
    "# Analyze each government period\n",
    "# for name, (start, end) in gov_periods.items():\n",
    "#     # Filter the data for the current government period\n",
    "#     mask = (df_clean['publish date'] >= start) & (df_clean['publish date'] < end)\n",
    "#     subset = df_clean[mask]\n",
    "\n",
    "#     # Count the number of explicit and non-explicit podcasts\n",
    "#     explicit_counts = subset['explicit'].value_counts()\n",
    "\n",
    "#     # Plot the counts\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     explicit_counts.plot(kind='barh', color=['blue', 'red'])\n",
    "#     plt.title(f'Explicitness Counts during {name}\\'s Presidency')\n",
    "#     plt.xlabel('Number of Podcasts')\n",
    "#     plt.ylabel('Explicitness')\n",
    "\n",
    "    \n",
    "# # --------- Required part to set X-axis upper limit to the highest freq\n",
    "#     # Set the x-axis limits\n",
    "#     plt.xlim(0, highest_freq)\n",
    "    \n",
    "#     # Set the xticks\n",
    "#     xticks = np.arange(0, highest_freq, 0.05 * 1e7)  # adjust the step size as needed\n",
    "#     plt.xticks(xticks)\n",
    "# # --------- Required part to set X-axis upper limit to the highest freq\n",
    "\n",
    "\n",
    "#     filename = f\"{name.replace(' ', '_')}_explicit_counts.png\"\n",
    "#     plt.savefig(filename)\n",
    "\n",
    "#     plt.close()\n",
    "    \n",
    "    \n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "# Plot 4: the most popular podcasts category of all time:\n",
    "\n",
    "    # Count the number of explicit and non-explicit podcasts\n",
    "# cat_counts = df_clean['category'].value_counts()\n",
    "# top_cat = cat_counts[:30]\n",
    "# # Plot the counts\n",
    "# plt.figure(figsize=(40, 10))\n",
    "\n",
    "# sns.barplot(x=top_cat.values, y=top_cat.index)\n",
    "# plt.title(f'30 Most common podcast topics since 1992-')\n",
    "# plt.xlabel('Number of Podcasts')\n",
    "# plt.ylabel('Topic')\n",
    "\n",
    "# filename = \"popular_categories_of_all_time.png\"\n",
    "# plt.savefig(filename)\n",
    "\n",
    "# plt.close()\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "# # Plot 5: time series plot that plots the number of podcasts by each category from 1990 - 2000\n",
    "# # Convert 'publish date' to datetime and extract the year\n",
    "# df_clean_copy = df_clean.copy()\n",
    "# df_clean_copy.loc[:, 'year'] = pd.to_datetime(df_clean_copy['publish date']).dt.year\n",
    "\n",
    "# # Filter the top 4 categories (only has 4 categories during that time)\n",
    "# top_categories = df_clean_copy['category'].value_counts().index[:5]\n",
    "# df_top_categories = df_clean_copy[df_clean_copy['category'].isin(top_categories)]\n",
    "\n",
    "# # Count the number of podcasts per category per year\n",
    "# df_counts = df_top_categories.groupby(['year', 'category']).size().reset_index(name='count')\n",
    "\n",
    "# # Create a new figure\n",
    "# plt.figure(figsize=(25, 10))\n",
    "\n",
    "# # Create a line plot using seaborn\n",
    "# sns.lineplot(x='year', y='count', hue='category', data=df_counts)\n",
    "\n",
    "# # Add labels and title\n",
    "# plt.title('Podcasts Trend by Year for Top 4 Categories from 1990 to 2000')\n",
    "# plt.xlabel('Year')\n",
    "# plt.ylabel('Podcasts Count')\n",
    "# plt.xlim(1990, 2000)\n",
    "# plt.yticks(np.arange(0, 200, 10))\n",
    "# plt.ylim(0, 200)\n",
    "\n",
    "# # Save the figure\n",
    "# plt.savefig('1990_2000_series_plot.png')\n",
    "\n",
    "# # Close the figure\n",
    "# plt.close()\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "# Plot 6: time series plot that plots the number of podcasts by each category from 2000 - 2010\n",
    "# Convert 'publish date' to datetime and extract the year\n",
    "# df_clean_copy = df_clean.copy()\n",
    "# df_clean_copy.loc[:, 'year'] = pd.to_datetime(df_clean_copy['publish date']).dt.year\n",
    "\n",
    "# # Filter the top 10 categories\n",
    "# top_categories = df_clean_copy['category'].value_counts().index[:10]\n",
    "# df_top_categories = df_clean_copy[df_clean_copy['category'].isin(top_categories)]\n",
    "\n",
    "# # Count the number of podcasts per category per year\n",
    "# df_counts = df_top_categories.groupby(['year', 'category']).size().reset_index(name='count')\n",
    "\n",
    "# # Create a new figure\n",
    "# plt.figure(figsize=(25, 10))\n",
    "\n",
    "# # Create a line plot using seaborn\n",
    "# sns.lineplot(x='year', y='count', hue='category', data=df_counts)\n",
    "\n",
    "# # Add labels and title\n",
    "# plt.title('Podcasts Trend by Year for Top 10 Categories from 2000 to 2010')\n",
    "# plt.xlabel('Year')\n",
    "# plt.ylabel('Podcasts Count')\n",
    "# plt.xlim(2000, 2010)\n",
    "# plt.yticks(np.arange(0, 7000, 200))\n",
    "# plt.ylim(0, 7000)\n",
    "\n",
    "# # Save the figure\n",
    "# plt.savefig('2000_2010_series_plot.png')\n",
    "\n",
    "# # Close the figure\n",
    "# plt.close()\n",
    "\n",
    "# # -------------------------------------------------------------------\n",
    "\n",
    "# # Plot 7: time series plot that plots the number of podcasts by each category from 2010 - 2023\n",
    "# # Convert 'publish date' to datetime and extract the year\n",
    "# df_clean_copy = df_clean.copy()\n",
    "# df_clean_copy.loc[:, 'year'] = pd.to_datetime(df_clean_copy['publish date']).dt.year\n",
    "\n",
    "# # Filter the top 10 categories\n",
    "# top_categories = df_clean_copy['category'].value_counts().index[:10]\n",
    "# df_top_categories = df_clean_copy[df_clean_copy['category'].isin(top_categories)]\n",
    "\n",
    "# # Count the number of podcasts per category per year\n",
    "# df_counts = df_top_categories.groupby(['year', 'category']).size().reset_index(name='count')\n",
    "\n",
    "# # Create a new figure\n",
    "# plt.figure(figsize=(25, 10))\n",
    "\n",
    "# # Create a line plot using seaborn\n",
    "# sns.lineplot(x='year', y='count', hue='category', data=df_counts)\n",
    "\n",
    "# # Add labels and title\n",
    "# plt.title('Podcasts Trend for Top 10 Categories from 2010 to 2023')\n",
    "# plt.xlabel('Year')\n",
    "# plt.ylabel('Podcasts Count')\n",
    "# plt.xlim(2010, 2023)\n",
    "# plt.yticks(np.arange(0, 500000, 20000))\n",
    "# plt.xticks(np.arange(2010, 2023, 1))\n",
    "# plt.ylim(0, 500000)\n",
    "\n",
    "# # Save the figure\n",
    "# plt.savefig('2010_2023_series_plot.png')\n",
    "\n",
    "# # Close the figure\n",
    "# plt.close()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Plot 8: time series plot that plots the number of podcasts by each category from 1990 - 2023\n",
    "# Convert 'publish date' to datetime and extract the year\n",
    "# df_clean_copy = df_clean.copy()\n",
    "# df_clean_copy.loc[:, 'year'] = pd.to_datetime(df_clean_copy['publish date']).dt.year\n",
    "\n",
    "# # Filter the top 10 categories\n",
    "# top_categories = df_clean_copy['category'].value_counts().index[:10]\n",
    "# df_top_categories = df_clean_copy[df_clean_copy['category'].isin(top_categories)]\n",
    "\n",
    "# # Count the number of podcasts per category per year\n",
    "# df_counts = df_top_categories.groupby(['year', 'category']).size().reset_index(name='count')\n",
    "\n",
    "# # Create a new figure\n",
    "# plt.figure(figsize=(25, 10))\n",
    "\n",
    "# # Create a line plot using seaborn\n",
    "# sns.lineplot(x='year', y='count', hue='category', data=df_counts)\n",
    "\n",
    "# # Add labels and title\n",
    "# plt.title('Podcasts Trend for Top 10 Categories from 1990 to 2023')\n",
    "# plt.xlabel('Year')\n",
    "# plt.ylabel('Podcasts Count')\n",
    "# plt.xlim(1990, 2023)\n",
    "# plt.yticks(np.arange(0, 500000, 10000))\n",
    "# plt.ylim(0, 500000)\n",
    "\n",
    "# # Save the figure\n",
    "# plt.savefig('1990_2023_series_plot.png')\n",
    "\n",
    "# # Close the figure\n",
    "# plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "9dc96023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot 5: time series plot that plots the number of podcasts by each category from 1990 - 2000\n",
    "# # Convert 'publish date' to datetime and extract the year\n",
    "# df_clean_copy = df_clean.copy()\n",
    "# df_clean_copy.loc[:, 'year'] = pd.to_datetime(df_clean_copy['publish date']).dt.year\n",
    "\n",
    "# # Filter the top 4 categories (only has 4 categories during that time)\n",
    "# top_categories = df_clean_copy['category'].value_counts().index[:5]\n",
    "# df_top_categories = df_clean_copy[df_clean_copy['category'].isin(top_categories)]\n",
    "\n",
    "# # Count the number of podcasts per category per year\n",
    "# df_counts = df_top_categories.groupby(['year', 'category']).size().reset_index(name='count')\n",
    "\n",
    "# # Create a new figure\n",
    "# plt.figure(figsize=(25, 10))\n",
    "\n",
    "# # Create a line plot using seaborn\n",
    "# sns.lineplot(x='year', y='count', hue='category', data=df_counts)\n",
    "\n",
    "# # Add labels and title\n",
    "# plt.title('Podcasts Trend by Year for Top 4 Categories from 1990 to 2000')\n",
    "# plt.xlabel('Year')\n",
    "# plt.ylabel('Podcasts Count')\n",
    "# plt.xlim(1990, 2000)\n",
    "# plt.yticks(np.arange(0, 200, 10))\n",
    "# plt.ylim(0, 200)\n",
    "\n",
    "# # Save the figure\n",
    "# plt.savefig('1990_2000_series_plot.png')\n",
    "\n",
    "# # Close the figure\n",
    "# plt.close()\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "# Plot 6: time series plot that plots the number of podcasts by each category from 2000 - 2010\n",
    "# Convert 'publish date' to datetime and extract the year\n",
    "# df_clean_copy = df_clean.copy()\n",
    "# df_clean_copy.loc[:, 'year'] = pd.to_datetime(df_clean_copy['publish date']).dt.year\n",
    "\n",
    "# # Filter the top 10 categories\n",
    "# top_categories = df_clean_copy['category'].value_counts().index[:10]\n",
    "# df_top_categories = df_clean_copy[df_clean_copy['category'].isin(top_categories)]\n",
    "\n",
    "# # Count the number of podcasts per category per year\n",
    "# df_counts = df_top_categories.groupby(['year', 'category']).size().reset_index(name='count')\n",
    "\n",
    "# # Create a new figure\n",
    "# plt.figure(figsize=(25, 10))\n",
    "\n",
    "# # Create a line plot using seaborn\n",
    "# sns.lineplot(x='year', y='count', hue='category', data=df_counts)\n",
    "\n",
    "# # Add labels and title\n",
    "# plt.title('Podcasts Trend by Year for Top 10 Categories from 2000 to 2010')\n",
    "# plt.xlabel('Year')\n",
    "# plt.ylabel('Podcasts Count')\n",
    "# plt.xlim(2000, 2010)\n",
    "# plt.yticks(np.arange(0, 7000, 200))\n",
    "# plt.ylim(0, 7000)\n",
    "\n",
    "# # Save the figure\n",
    "# plt.savefig('2000_2010_series_plot.png')\n",
    "\n",
    "# # Close the figure\n",
    "# plt.close()\n",
    "\n",
    "# # -------------------------------------------------------------------\n",
    "\n",
    "# # Plot 7: time series plot that plots the number of podcasts by each category from 2010 - 2023\n",
    "# # Convert 'publish date' to datetime and extract the year\n",
    "# df_clean_copy = df_clean.copy()\n",
    "# df_clean_copy.loc[:, 'year'] = pd.to_datetime(df_clean_copy['publish date']).dt.year\n",
    "\n",
    "# # Filter the top 10 categories\n",
    "# top_categories = df_clean_copy['category'].value_counts().index[:10]\n",
    "# df_top_categories = df_clean_copy[df_clean_copy['category'].isin(top_categories)]\n",
    "\n",
    "# # Count the number of podcasts per category per year\n",
    "# df_counts = df_top_categories.groupby(['year', 'category']).size().reset_index(name='count')\n",
    "\n",
    "# # Create a new figure\n",
    "# plt.figure(figsize=(25, 10))\n",
    "\n",
    "# # Create a line plot using seaborn\n",
    "# sns.lineplot(x='year', y='count', hue='category', data=df_counts)\n",
    "\n",
    "# # Add labels and title\n",
    "# plt.title('Podcasts Trend for Top 10 Categories from 2010 to 2023')\n",
    "# plt.xlabel('Year')\n",
    "# plt.ylabel('Podcasts Count')\n",
    "# plt.xlim(2010, 2023)\n",
    "# plt.yticks(np.arange(0, 500000, 20000))\n",
    "# plt.xticks(np.arange(2010, 2023, 1))\n",
    "# plt.ylim(0, 500000)\n",
    "\n",
    "# # Save the figure\n",
    "# plt.savefig('2010_2023_series_plot.png')\n",
    "\n",
    "# # Close the figure\n",
    "# plt.close()\n",
    "\n",
    "# # -------------------------------------------------------------------\n",
    "\n",
    "# # Plot 8: time series plot that plots the number of podcasts by languages from 1990 - 2023\n",
    "# # Convert 'publish date' to datetime and extract the year\n",
    "df_clean_copy = df_clean.copy()\n",
    "df_clean_copy.loc[:, 'year'] = pd.to_datetime(df_clean_copy['publish date']).dt.year\n",
    "\n",
    "# Filter the top 15 languages\n",
    "top_categories = df_clean_copy['language'].value_counts().index[:15]\n",
    "df_top_categories = df_clean_copy[df_clean_copy['language'].isin(top_categories)]\n",
    "\n",
    "# Count the number of podcasts per category per year\n",
    "df_counts = df_top_categories.groupby(['year', 'language']).size().reset_index(name='count')\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "# Create a line plot using seaborn\n",
    "sns.lineplot(x='year', y='count', hue='language', data=df_counts)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Top 10 Languages from 1990 to 2023')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Podcast Counts')\n",
    "plt.xlim(1990, 2023)\n",
    "plt.yticks(np.arange(0, 3000000, 100000))\n",
    "plt.ylim(0, 3000000)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('1990_2023_lang_series_plot.png')\n",
    "\n",
    "# Close the figure\n",
    "plt.close()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Plot 9: time series plot that plots the number of podcasts by explicit status from 1990 - 2023\n",
    "# Convert 'publish date' to datetime and extract the year\n",
    "# df_clean_copy = df_clean.copy()\n",
    "# df_clean_copy.loc[:, 'year'] = pd.to_datetime(df_clean_copy['publish date']).dt.year\n",
    "\n",
    "# # Filter the top 2 explicit statuses\n",
    "# top_categories = df_clean_copy['explicit'].value_counts().index[:2]\n",
    "# df_top_categories = df_clean_copy[df_clean_copy['explicit'].isin(top_categories)]\n",
    "\n",
    "# # Count the number of podcasts per category per year\n",
    "# df_counts = df_top_categories.groupby(['year', 'explicit']).size().reset_index(name='count')\n",
    "\n",
    "# # Create a new figure\n",
    "# plt.figure(figsize=(25, 10))\n",
    "\n",
    "# # Create a line plot using seaborn\n",
    "# sns.lineplot(x='year', y='count', hue='explicit', data=df_counts)\n",
    "\n",
    "# # Add labels and title\n",
    "# plt.title('Explicit Podcasts from 1990 to 2023')\n",
    "# plt.xlabel('Year')\n",
    "# plt.ylabel('Podcast Counts')\n",
    "# plt.xlim(1990, 2023)\n",
    "# plt.yticks(np.arange(0, 5000000, 100000))\n",
    "# plt.ylim(0, 5000000)\n",
    "\n",
    "# # Save the figure\n",
    "# plt.savefig('1990_2023_explicit_series_plot.png')\n",
    "\n",
    "# # Close the figure\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e52eaeee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['podcast title', 'publish date', 'category', 'language', 'explicit'], dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f8798128",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv(\"df_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8e8d8f9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-82506b823f4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Initialize a dictionary to store the counts of each pair of categories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcatsDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextractPairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "df_no_nat = df_clean.dropna(subset=['category'])\n",
    "\n",
    "catsList = df_no_nat[\"category\"].str.split(',')\n",
    "\n",
    "# Initialize a dictionary to store the counts of each pair of categories\n",
    "catsDict = defaultdict(int)\n",
    "\n",
    "def extractPairs(item): \n",
    "    for i in range(len(item)): \n",
    "        for j in range(i+1, len(item)): \n",
    "            # Sort the pair so that 'A, B' and 'B, A' are considered the same\n",
    "            pair = tuple(sorted([item[i], item[j]]))\n",
    "            catsDict[pair] += 1\n",
    "\n",
    "# Extract all pairs from the categories list\n",
    "for item in catsList: \n",
    "    extractPairs(item)\n",
    "\n",
    "df_pairs = pd.DataFrame.from_dict(catsDict, orient='index', columns=['count']).reset_index()\n",
    "\n",
    "# Create two separate columns for each element of the pair\n",
    "df_pairs[['Category1', 'Category2']] = pd.DataFrame(df_pairs['index'].tolist(), index=df_pairs.index)\n",
    "\n",
    "# Drop the original 'index' column\n",
    "df_pairs = df_pairs.drop(columns=['index'])\n",
    "\n",
    "# Create a pivot table for the heatmap\n",
    "pivot_table = df_pairs.pivot(index='Category1', columns='Category2', values='count').fillna(0)\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "sns.heatmap(pivot_table, cmap='YlGnBu')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Heatmap of Podcast Category Pairs')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Category')\n",
    "\n",
    "# Save the figure to a file and do not display it\n",
    "plt.savefig('heatmap.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "12d2a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #boilerplate code for subplots \n",
    "# fig, ax = plt.subplots(2, 2, figsize=(12,12))\n",
    "\n",
    "# #replace this with whatever your end up plotting\n",
    "# #ax[0][0] just says which subplot to plot onto \n",
    "# #ax[0][0].scatter([1,2,3],[1,2,3])\n",
    "\n",
    "# #or with seaborn use \n",
    "# #sns.heatmap(pivot_table, ax=ax[0][0])\n",
    "\n",
    "# ax[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5792cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pairs.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "256d2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 correlated keywords (Haven't figured out)\n",
    "# catList = df_clean[\"category\"].str.split(',')\n",
    "\n",
    "# # Initialize a dictionary to store the counts of each pair of categories\n",
    "# catDict = defaultdict(int)\n",
    "\n",
    "# def extractPairs(item): \n",
    "#     for i in range(len(item)): \n",
    "#         for j in range(i+1, len(item)): \n",
    "#             # Sort the pair so that 'A, B' and 'B, A' are considered the same\n",
    "#             pair = tuple(sorted([item[i], item[j]]))\n",
    "#             catsDict[pair] += 1\n",
    "\n",
    "# # Extract all pairs from the categories list\n",
    "# for item in catList: \n",
    "#     if isinstance(item, list):  # Add this line to check if item is a list before calling extractPairs\n",
    "#         extractPairs(item)\n",
    "\n",
    "# # Convert the dictionary to a DataFrame and reset index\n",
    "# df_pair = pd.DataFrame.from_dict(catDict, orient='index', columns=['count']).reset_index()\n",
    "\n",
    "# # Create two separate columns for each element of the pair\n",
    "# df_pair['Category1'] = df_pair['index'].apply(lambda x: x[0])\n",
    "# df_pair['Category2'] = df_pair['index'].apply(lambda x: x[1])\n",
    "\n",
    "# # Drop the original 'index' column\n",
    "# df_pair = df_pair.drop(columns=['index'])\n",
    "\n",
    "# # Create a pivot table for the heatmap\n",
    "# pivot_table = df_pair.pivot(index='Category1', columns='Category2', values='count').fillna(0)\n",
    "\n",
    "# # Calculate correlation matrix\n",
    "# corr_matrix = pivot_table.corr()\n",
    "\n",
    "# # Unstack correlation matrix and sort by correlation coefficient\n",
    "# corr_pairs = corr_matrix.unstack().sort_values(ascending=False)\n",
    "\n",
    "# # Get top 20 most correlated pairs (excluding self-correlations)\n",
    "# top_corr_pairs = corr_pairs[corr_pairs != 1.0][:20]\n",
    "\n",
    "# # Create a new DataFrame for these pairs and their correlations\n",
    "# df_top_corr = pd.DataFrame(top_corr_pairs.index.tolist(), columns=['level_0', 'level_1'])\n",
    "# df_top_corr['correlation'] = top_corr_pairs.values\n",
    "\n",
    "# # Pivot this DataFrame to create a table suitable for a heatmap\n",
    "# pivot_table_top_corr = df_top_corr.pivot(index='level_0', columns='level_1', values='correlation')\n",
    "\n",
    "# # Create a new figure\n",
    "# plt.figure(figsize=(10, 10))\n",
    "\n",
    "# # Create a heatmap using seaborn\n",
    "# sns.heatmap(pivot_table_top_corr, cmap='YlGnBu')\n",
    "\n",
    "# # Add labels and title\n",
    "# plt.title('Heatmap of Top 20 Most Correlated Podcast Category Pairs')\n",
    "# plt.xlabel('Category')\n",
    "# plt.ylabel('Category')\n",
    "\n",
    "# # Save the figure to a file and do not display it\n",
    "# plt.savefig('heatmap.png')\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0bfb813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot an empirical-cdf graph regarding category\n",
    "# df_no_nat_cat = df_clean.dropna(subset=['category'])\n",
    "# top_categories = df_no_nat_cat['category'].value_counts().index[:10]\n",
    "\n",
    "# # Create a new figure\n",
    "# plt.figure(figsize=(14, 10))\n",
    "\n",
    "# # Calculate and plot the ECDF for each of the top categories\n",
    "# for category in top_categories:\n",
    "#     data = df_no_nat_cat[df_no_nat_cat['category'] == category]['category']  \n",
    "#     x = np.sort(data)\n",
    "#     y = np.arange(1, len(x)+1) / len(x)\n",
    "#     sns.lineplot(x=x, y=y, label=category)\n",
    "\n",
    "# # Add labels and title\n",
    "# plt.title('Empirical Cumulative Distribution Function (ECDF) by Category')\n",
    "# plt.xlabel('Value')\n",
    "# plt.ylabel('Cumulative Probability')\n",
    "\n",
    "# # Add a legend\n",
    "# plt.legend(title='Category')\n",
    "\n",
    "# # rotate x axis so that labels won't overlap \n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "# # Save the figure to a file and do not display it\n",
    "# plt.savefig('emp_cdf_cat.png')\n",
    "\n",
    "# # Close the figure to prevent it from displaying in your notebook or script output\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c75357e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ECDF plot, the dates of the first episode on the x axis\n",
    "# data = pd.to_datetime(df_clean['publish date']).dt.year\n",
    "\n",
    "# # Calculate the ECDF\n",
    "# x = np.sort(data)\n",
    "# y = np.arange(1, len(x)+1) / len(x)\n",
    "\n",
    "# # Create a new figure\n",
    "# fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# # Create a line plot for the ECDF using seaborn\n",
    "# sns.lineplot(x=x, y=y, ax=ax)\n",
    "\n",
    "# # Limit the number of x-labels to 30\n",
    "# ax.xaxis.set_major_locator(MaxNLocator(nbins=30))\n",
    "\n",
    "# # Add labels and title\n",
    "# plt.title('Empirical Cumulative Distribution Function (ECDF) of Podcast Age')\n",
    "# plt.xlabel('Year of First Episode')\n",
    "# plt.ylabel('Cumulative Proportion')\n",
    "\n",
    "# plt.xticks(rotation=45)\n",
    "# # Show the plot\n",
    "# plt.savefig(\"ecdf_first_episode.png\")\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "8c509500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean['podcast title'][0:5]\n",
    "# df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "755b086d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sound Design Live - Career building interviews on live sound, theatre, AV, recording, and sound system tuning\n"
     ]
    }
   ],
   "source": [
    "# print(consistent_sorted_podcasts[consistent_sorted_podcasts['min'] == '2007-07-06 15:30:06'])\n",
    "# print(consistent_sorted_podcasts[consistent_sorted_podcasts['median'] == '2017-07-31 21:59:35.00']['podcast title'].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aac129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please don't run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort podcasts by time span\n",
    "# sorted_podcasts_by_time_span = grouped.sort_values('time span', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5320b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_podcasts_by_time_span.to_csv('sorted_podcasts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9319cd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
